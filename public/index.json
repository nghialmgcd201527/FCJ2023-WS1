[
{
	"uri": "/2-prerequiste/2.1-createcloud9workspace/",
	"title": "Create a Cloud9 Workspace",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Workspace Type Cloud9 in the service search bar on the AWS Console then select Cloud9. Click Create environment Name the Cloud9 Workspace serverless-workshop. In the Description field, enter the purpose you want to use in this workspace. Enter deploy \u0026ldquo;Todos app\u0026rdquo; Environment type, here we will create a server to run this Cloud9 workspace. I will choose New EC2 instance option to create a new server. Go to settings for New EC2 instance, select Additional instance types then we choose t3.medium Leave defaults for other settings. Click the Create button Wait about 10 minutes for Cloud9 Workspace to be created. Once the Cloud9 Workspace is created, we will have an environment to work with the AWS CLI and other tools. In the list of Environments created, find the serverless-workshop environment and click the Open button to open the Cloud9 environment. After the environment opens, let\u0026rsquo;s close the sections below that were initialized at the start and create a new terminal page. Our workspace will look like this.\n"
},
{
	"uri": "/3-serverlessbackend/3.1-dynamodb/",
	"title": "Define a DynamoDB table",
	"tags": [],
	"description": "",
	"content": "What is Amazon DynamoDB? Amazon DynamoDB is a serverless key-value pair and a type of non-relational (Non-SQL) database that delivers millisecond performance at all sizes.\nSimilar to other databases, Amazon DynamoDB stores data in tables. In our application, we will store the information of the tasks in the table of DynamoDB. This table will be accessed by the Lambda function in response to the API from our web application.\nWe will use SAM to initialize the DynamoDB table.\nAdd the DynamoDB table to the SAM template The line AWS:DynamoDB::Table is used to define the DynamoDB table.\nGo to the template.yaml file in the sam folder, add the following code in the Resource section and below the MyAuthFunction function.\n# Create DynamoDB table\rTasksTable:\rType: AWS::DynamoDB::Table\rProperties:\rAttributeDefinitions:\r- AttributeName: \u0026#34;user\u0026#34;\rAttributeType: \u0026#34;S\u0026#34;\r- AttributeName: \u0026#34;id\u0026#34;\rAttributeType: \u0026#34;S\u0026#34;\rKeySchema:\r- AttributeName: \u0026#34;user\u0026#34;\rKeyType: \u0026#34;HASH\u0026#34;\r- AttributeName: \u0026#34;id\u0026#34;\rKeyType: \u0026#34;RANGE\u0026#34;\rBillingMode: PAY_PER_REQUEST First you will have AttributeDefinitions where the attributes (columns) are defined in the DynamoDB table. In our table, there are 2 attributes defined as users and id. Both are of type string (AttributeType: \u0026quot;S\u0026quot;).\nKeySchema defines the primary key of the table. The primary key of the table is a combination of the two attributes user and id. user is the hash key (KeyType: \u0026quot;HASH\u0026quot;) and id is the range key (KeyType: \u0026quot;RANGE\u0026quot;). This indicates that the table will be managed, partitioned based on the user and id attributes.\nBillingMode: PAY_PER_REQUEST defines how to pay for that table as payment on demand.\nThe syntax in YAML is space sensitive, so make sure the scope of the TasksTable function is indented deeper than the scope of Resources.\nWe will have the result as shown below.\nFor more information about AWS::Serverless::Table, refer to SAM\u0026rsquo;s resource here.\n"
},
{
	"uri": "/5-deploy/5.1-deployserverless/",
	"title": "Deploy the Serverless Application",
	"tags": [],
	"description": "",
	"content": "In this step, we will use the SAM CLI to build and deploy the serverless application stack.\nFirst, we need to ensure we\u0026rsquo;re using the latest version of the SAM CLI:\npip install --user --upgrade aws-sam-cli We will see the output as shown.\nNext, go to the sam: folder\ncd ~/environment/serverless-tasks-webapp/sam Run the following command to build the application:\nsam build When the build is successful, we will have as shown.\nThe sam build command processes your AWS SAM template file, application code, and any applicable language-specific files and dependencies. The command also copies build artifacts in the format and location expected for subsequent steps in your workflow.\nNow we can deploy the application:\nsam deploy --guided This command packages and deploys the application to your AWS account. It provides a series of prompts:\nFirst, it will ask us for the stack name, we can name it any, here we set it as tasks-app. Then it will choose the region we want to deploy the application to, here we choose ap-southeast-1.\nNext, we will type y and press Enter to confirm deploy.\nTo allow SAM to create roles to connect to resources in our template, let\u0026rsquo;s allow SAM CLI to create IAM roles, we type Y and press Enter.\nTo disable rollback to protect our resources when the declaration fails, we type y and press Enter.\nTo allow arguments to be saved to the configuration file, we enter Y and press Enter.\nSince the SAM configuration file does not exist yet, we create it, we press Enter to default, similar to the environment of SAM configuration.\nCheck the information before deploying.\nIf the deploy is successful, we will see the output as shown.\nPlease make a note of the TasksApi endpoint URL and S3BucketName values; you will need these in subsequent steps.\n"
},
{
	"uri": "/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "The application architecture uses AWS Lambda, Amazon API Gateway, Amazon DynamoDB, Amazon Simple Storage Service (S3), and AWS Amplify Console. Amplify Console provides continuous deployment and hosting of the static web resources including HTML, CSS, JavaScript, and image files which are loaded in the user\u0026rsquo;s browser. JavaScript executed in the browser sends and receives data from a public backend API built using Lambda and API Gateway. DynamoDB provides a persistence layer where data can be stored by the API\u0026rsquo;s Lambda function. S3 is used to store uploaded images. Finally, Amazon Rekognition is used to detect and label objects in those images.\n"
},
{
	"uri": "/",
	"title": "Session Management",
	"tags": [],
	"description": "",
	"content": "Deploy \u0026ldquo;Todo app\u0026rdquo; with Serverless Overall In this workshop, we will create a “Todo app” using Serverless and API to store and retrieve data in the Cloud. In addition, we will combine Machine Learning to identify keywords related to the images that you upload for each Task.\nContent Introduction Prerequisites Build a serverless backend: AWS Lambda and AWS SAM Configure API authorization: API Gateway Build and deploy a web application: AWS Amplify Test the application Configure image metadata extraction: Amazon Rekognition Terminate Resources "
},
{
	"uri": "/6-test/6.1-dynamodb/",
	"title": "Viewing the data in DynamoDB",
	"tags": [],
	"description": "",
	"content": " Type DynamoDB in the service search bar on the AWS Console then select DynamoDB. In the left navigation bar, select Tables. Click on the table whose name includes the name of our application tasks-app-TasksTable. This is the table created by the SAM template. Click the Explore table items button. We will see the data in the table in Items returned.\nClicking on the partition key (user) of that table will take you to where to edit the information of that item.\nWhen you change data in DynamoDB, we need to refresh the application\u0026rsquo;s web page to see the change.\n"
},
{
	"uri": "/5-deploy/5.2-configapigateway/",
	"title": "Configure the API Gateway endpoint",
	"tags": [],
	"description": "",
	"content": "After running the sam deploy --guided command in the previous step, you will get the URL of the API Gateway endpoint.\nThe web application requires configuration for this endpoint so that it knows where to send requests.\nCopy the URL of the API Gateway endpoint of our application, go to the /webapp/src directory, open the file main.js. Paste that URL into the value of axios.defaults.baseURL as shown.\nNote that you copied and included the name of the stage /v1 in that URL and removed the / at the end of the URL.\nThe line Vue.config.productionTip = false will disable the tips displayed in Vue console, it will optimize in production environment.\nNext, we will set the default URL for all Axios requests. It will be the endpoint URL where the API request is sent to.\nIn general, the code above initializes the Vue.js application by creating a Vue instance, configuring the base URL for Axios requests. It also turns off the tips displayed in the Vue console.\n"
},
{
	"uri": "/3-serverlessbackend/3.2-lambdafunction/",
	"title": "Create a Lambda function",
	"tags": [],
	"description": "",
	"content": "What is AWS Lambda? AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. Lambda automatically allocates compute power and runs your code based on the incoming request or event, for any scale of traffic.\nHow it works:\nUpload your code to AWS Lambda or write code in Lambda\u0026rsquo;s code editor. (In this workshop, we\u0026rsquo;ll be writing code and uploading it using SAM.) Set up your code to trigger from other AWS services, HTTP endpoints, or in-app activity. Lambda runs your code only when triggered, using only the compute resources needed. Just pay for the compute time you use. Anatomy of a Lambda function The Lambda function handler is the method in your function code that processes events. When your function is invoked, Lambda runs the handler method. When the handler exits or returns a response, it becomes available to handle another event.\nExample Lambda function structure:\nexports.handler = async (event) =\u0026gt; {\r// TODO implement\rconst response = {\rstatusCode: 200,\rbody: JSON.stringify(\u0026#39;Hello from Lambda!\u0026#39;),\r};\rreturn response;\r}; Here, the event is the incoming request. The response is the outgoing response.\nCreate a Lambda function Go to the path sam/src/handlers/createTask and select the file named app.js, copy and paste the following code into the file:\nconst { DynamoDBClient } = require(\u0026#39;@aws-sdk/client-dynamodb\u0026#39;)\rconst { DynamoDBDocumentClient, PutCommand } = require(\u0026#39;@aws-sdk/lib-dynamodb\u0026#39;)\rconst uuid = require(\u0026#39;uuid\u0026#39;)\rconst ddbClient = new DynamoDBClient()\rconst ddbDocClient = DynamoDBDocumentClient.from(ddbClient)\rconst tableName = process.env.TASKS_TABLE\rexports.handler = async (event) =\u0026gt; {\rconsole.info(\u0026#39;received:\u0026#39;, event)\rconst body = JSON.parse(event.body)\rconst user = event.requestContext.authorizer.principalId\rconst id = uuid.v4()\rconst title = body.title\rconst bodyText = body.body\rconst createdAt = new Date().toISOString()\rlet dueDate = createdAt\rif (\u0026#39;dueDate\u0026#39; in body) {\rdueDate = body.dueDate\r}\rconst params = {\rTableName: tableName,\rItem: { user: `user#${user}`, id: `task#${id}`, title: title, body: bodyText, dueDate: dueDate, createdAt: createdAt }\r}\rconsole.info(`Writing data to table ${tableName}`)\rconst data = await ddbDocClient.send(new PutCommand(params))\rconsole.log(\u0026#39;Success - item added or updated\u0026#39;, data)\rconst response = {\rstatusCode: 200,\rheaders: {\r\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;\r},\rbody: JSON.stringify(data)\r}\rreturn response\r} This code imported the AWS SDK library for DynamoDB, the uuid library for identification, and installed the DynamoDB client.\nLambda function is initialized with exports.handler, which acts as an entry point of the function. It takes an envent object as an input parameter and returns a response. object\nIt creates a params object with the necessary elements, including the name of the DynamoDB table, the properties of the item in the table (user, id, title, bodyText, dueDate, createdAt).\nThe function logs data to the DynamoDB table with PutCommand and logs a success message.\nResponse will return an object with statusCode of 200, set headers with CORS and body as the logged data.\nIn short, this code serves to create and update tasks in DynamoDB table based on sending API request. It leverages the AWS SDK for DynamoDB, Node.js, and AWS Lambda to provide a scalable and serverless task management solution.\nAdd the Lambda function to the SAM template Copy and paste the code below into the Resource section of the file template.yml, after the function TasksTable.\nThe syntax in YAML is space sensitive, so make sure the scope of the CreateTaskFunction function is indented deeper than the scope of Resources.\nThe value AWS::Serverless::Function is used to initialize the Lambda function. The CodeUri attribute is used to specify the location of the app.js file in the src/handlers/createTask directory.\n# CreateTask Lambda Function\rCreateTaskFunction:\rType: AWS::Serverless::Function\rProperties:\rCodeUri: src/handlers/createTask\rHandler: app.handler\rPolicies:\r- DynamoDBCrudPolicy:\rTableName: !Ref TasksTable\rEnvironment:\rVariables:\rTASKS_TABLE: !Ref TasksTable\rEvents:\rPostTaskFunctionApi:\rType: Api\rProperties:\rRestApiId: !Ref TasksApi\rPath: /tasks\rMethod: POST\rAuth:\rAuthorizer: MyLambdaTokenAuthorizer Details and uses of the attributes declared above can be reviewed in this.\nDo as shown below.\n"
},
{
	"uri": "/2-prerequiste/2.2-ensurenodejsversion/",
	"title": "Ensure Node.js Version",
	"tags": [],
	"description": "",
	"content": "Cloud9 has Node.js pre-installed but it may not be compatible with the time I do this workshop. To ensure this, install Node.js v16 (codename Gallium) by running the following command in the Cloud9 terminal.\nLet\u0026rsquo;s check the current Node.js version running on Cloud9 with the following command:\nnode --version This is the version of Node.js that I am using in this workshop.\nIf the version of Node.js you are using is v16.x then you can skip this step and go to Clone Git repository. If the version you are using is different from the above then follow along Follow these steps to install the right version of Node.js for this workshop using the terminal page you just created.\nAt the terminal page on Cloud9, run the following command to make sure you have the latest version of Node.js Version Manager (nvm) installed (At the time I wrote the workshop, it was version 0.39.0).\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.2/install.sh | bash Next we will install Node.js v16 \u0026ldquo;Gallium\u0026rdquo;:\nnvm install \u0026#39;lts/gallium\u0026#39; Finally, we will choose the above Node.js version as the default version:\nnvm alias default \u0026#39;lts/gallium\u0026#39; Review the Node.js instance running on Cloud9 with the following command:\nnode --version The result we need is to start with v16.\n"
},
{
	"uri": "/6-test/6.2-logandmonitor/",
	"title": "Logging and Monitoring",
	"tags": [],
	"description": "",
	"content": "Logging Centralized logging gives us two important benefits. First, the logged logs are stored in a single place and formatted according to a certain standard, making it simple to manage tasks and analyze logs. Second, it provides us with a safe place to store the data of our logs.\nIn AWS Lambda, the default logging service is Amazon CloudWatch.\nLambda automatically passes information about function calls, along with logs and output from our functions\u0026rsquo; code, to CloudWatch Logs.\nLog groups are a standard part of CloudWatch and are used to categorize logs. Any logs generated by a Lambda function have a naming convention of /aws/lambda/function-name. A log group is a collection of log streams, which you can view in detail in the CloudWatch console.\nType CloudWatch in the service search bar on the AWS Console then select CloudWatch. In the left navigation bar, select Log groups. Select the log group with the CreateTaskFunction function. Each instance of a Lambda function has its own stream log. If a function is extended, the new version also has its own log stream. Every time an environment is selected for execution and a new environment is created to accommodate a certain amount of invocation and it creates a new log stream. How to name the log streams YYYY/MM//DD [Function version] [Execution environment GUID]\nFor example, here is the log stream of the CreateTaskFunction function.\nIn these logs there are:\nRequestId: is a unique ID generated for each function call. If the Lambda function calls the request again, this ID will remain unchanged and remain in the logs for each subsequent callback. Start/End: these values mark an invocation, so the log lines between these values all belong to the same invocation. Duration: execution time of the function. Billed Duration: rounding operation for Duration. Memory Size: the size of memory allocated to the function. Max Memory Used: Maximum memory size used during function execution. Init Duration: time since running INIT. Monitoring Metrics are metrics data at different time intervals (time series data) and service-level metrics (request rate, error rate, duration, CPU, etc). Lambda automatically publishes a set of metrics for Lambda functions.\nTo track and observe Lambda functions, the most important metrics are:\nErrors: Whether the errors are caused by logic or runtime in the code or by the error interaction with the Lambda service or other services. These can all be caused by other factors, such as not being granted permissions or exceeding the resource limit. Excution time: Measuring response time gives us only a limited view of performance in distributed applications. It\u0026rsquo;s important to capture and track performance at percentage intervals (such as 95% and 99%) to measure performance for at least 5% and 1% of requests. Throttling: Serverless application using extensible resources with Service Quotas for customer security. Throttling can tell us that quotas are set incorrectly, there is an error in the structure of the system, or the amount of traffic exceeds the limit. All Lambda functions automatically integrate with CloudWatch. Lambda automatically records many metrics, it is always published to CloudWatch metrics. If you direct to a function in Lambda, the Monitor section of the Metrics section gives us a quick look at the CloudWatch metrics integrated with a function.\nTo access the Monitor, go to the Lambda console, select the Lambda function you want to view. In the Monitor, tab in the Metrics section, we can see the metrics of the function. See illustration in the picture below.\nNext, we\u0026rsquo;ll add new functionality to our web application with Amazon Rekognition.\n"
},
{
	"uri": "/2-prerequiste/",
	"title": "Prerequisite",
	"tags": [],
	"description": "",
	"content": "\rRemember that Cloud9 workspaces should only be created by an IAM user (or assigned to an appropriate IAM role) with Admin privileges, not a root user.\nCloud9 Workspace We often use the Integrated Development Environment (IDE) locally, in this workshop we will use Cloud9. It is an IDE that runs in the cloud using a browser, including the important, essential features in the local IDE that we often use such as writing, running, debugging code. Cloud9 already comes with bundles of files like JavaScript, Python, NodeJS and others here\nFor AWS services\u0026rsquo; faster response, we recommend you to select the nearest Region during this workshop.\nRegion Support for Amazon Rekognition Note that not all services are available in all regions. Later in this workshop, we will be using Amazon Rekognition, which is available only in the following regions::\nUS East (Ohio) us-east-2 US East (N. Virginia) us-east-1 US West (N. California) us-west-1 US West (Oregon) us-west-2 Asia Pacific (Mumbai) ap-south-1 Asia Pacific (Seoul) ap-northeast-2 Asia Pacific (Singapore) ap-southeast-1 Asia Pacific (Sydney) ap-southeast-2 Asia Pacific (Tokyo) ap-northeast-1 Canada (Central) ca-central-1 Europe (Frankfurt) eu-central-1 Europe (Ireland) eu-west-1 Europe (London) eu-west-2 See more Amazon Rekognition latest updates.\nContent Create a Cloud9 Workspace Ensure Node.js Version Clone the Git repository and avoid free space issues with Cloud9 Install Amplify CLI "
},
{
	"uri": "/3-serverlessbackend/",
	"title": "Build a serverless backend: AWS Lambda and AWS SAM",
	"tags": [],
	"description": "",
	"content": "Overview In this section, we will use the Serverless Application Model (SAM) to build a backend process to handle requests from the web application. The application that we are going to deploy here allows users to create todo tasks and assign files to those tasks. To meet those requirements, JavaScript running in the browser needs to rely on a service in the cloud.\nYou will use a Lambda function so that every time a user creates a task, it will be called to execute. This function will store the task to DynamoDB, then will send the response to the front-end and update the new task on the front-end.\nThe function is called from the browser using Amazon API Gateway. You will implement this connection later. In this section, you will only test your function in isolation.\nWhat is SAM? Serverless Application Model (SAM) is an open-source framework that will make serverless application deployment easier. It provides a simple way to define serverless applications, and provides a set of tools for deploying those applications.\nIt allows us to specify the requirements of the application in code. SAM transforms and extends the SAM syntax to AWS Cloudformation to deploy your applications. You will see and use SAM templates throughout this workshop.\nHow does SAM work? AWS SAM is based on AWS Cloudformation. A serverless application is defined with a CloudFormation template and deployed with the CloudFormation stack. In short, the AWS SAM template is a CloudFormation template.\nAWS SAM defines a set of resources that describe common components of a serverless application. In order for AWS SAM to have objects defined in the CloudFormation template, the template must include a Transform section in the document root of AWS::Serverless-2016-10-31.\nOur sample application: Tasks API The API Tasks that we will build in this workshop include Amazon API Gateway HTTP endpoints to trigger AWS Lambda functions, which will read and write data to the Amazon DynamoDB database. The SAM template of the Tasks API will include a DynamoDB table, Lambda functions to list, view, and update Tasks in the table.\nIn this section, you will work with a Lambda function to create a new task. Tasks API components are defined in template.yaml. Next we will go into detail about the structure of the Lambda function.\nAnatomy of SAM template Go to the serverless-tasks-web folder followed by the sam folder then open the template.yaml file. Below is a piece of code in the SAM template file that lists the tasks:\nThese are the properties defined in the resource AWS:Serverless:Function, we will start to learn about them.\nFunctionName\nThe FunctionName property is optionally named for the Lambda function. If we don\u0026rsquo;t name it, CloudFormation will use the name of CloudFormation Stack, CloudFormation Resource and random ID.\nCodeUri\nThe CodeUri attribute specifies the path to the source code of the Lambda function in the workspace associated with the SAM template in the sam/ path. In this example, the Code snippet will be in the path src/handlers/ and getTasks will be its function.\nHandler\nThe Handler attribute specifies the entry point for the Lambda function. For Javascript, it will be formatted as file.function, where file is the filename containing the function without the extension .js belonging to the path CodeUri defined above and function is the name of the function in that file that will be executed when the Lambda function is called.\nEnvironment\nThe Environment attribute defines an environment variable, which will be available in the Lambda function. In this example, we are defining an environment variable TASKS_TABLE with a value of TasksTable. This environment variable will be used in the Lambda function to access the DynamoDB table where the tasks are stored.\nEvents\nThe Events attribute defines the events that will trigger the Lambda function when called. Event API is integrated with Lambda function and API Gateway endpoint, however SAM only supports Lambda function triggers from the following sources .\nThe event API is used to view the details of a Task defined in the RESTful resource /tasks and accessed using the HTTP GET method. SAM will convert API event to API Gateways to call Lambda function.\nContent Define a DynamoDB table Create a Lambda function "
},
{
	"uri": "/2-prerequiste/2.3-clonerepositoryandavoidingfreespace/",
	"title": "Clone the Git repository and avoid free space issues with Cloud9",
	"tags": [],
	"description": "",
	"content": "At the terminal page on Cloud9, run the following command to clone the Git repository serverless-tasks-webapp:\ngit clone https://github.com/aws-samples/serverless-tasks-webapp After running the above command, we will get as shown. We should see the serverless-tasks-webapp folder created.\nAvoid Cloud9 Free Space Issues By default, the free space of a Cloud9 instance is only about 2GB. Use the script below to avoid running out of space and problems during the workshop.\nFirst, we will update to the latest version of the AWS CLI:\npip install --user --upgrade awscli aws-sam-cli If the update is successful, we will see as shown below:\nGo to the serverless-tasks-webapp directory, check the size of the current volume with the following command:\ncd serverless-tasks-webapp\rdf -h We will get the result as shown below:\nfilesystem at the path /dev/nvme0n1p1 is the volume we are using. We will see that the free space of this volume is 3.5G. To avoid running out of space during the workshop, we will increase the capacity of this volume to 20G.\nIncrease Amazon EBS volume size to 20G:\nbash resize.sh 20 We will see output like this:\ndf -h Now check the size of the current volume with the following command:\nAs we can see in the path /dev/nvme0n1p1, the free space of the current volume has increased to 14G.\n"
},
{
	"uri": "/5-deploy/5.3-initializeamplify/",
	"title": "Initialize Amplify",
	"tags": [],
	"description": "",
	"content": "It\u0026rsquo;s time to set up Amplify so that we can create the necessary backend services needed to support the app. In this step, you\u0026rsquo;ll configure Amplify hosting.\nEnsure you are in the root of the web project (/webapp):\ncd ~/environment/serverless-tasks-webapp/webapp Initialize Amplify:\namplify init We will name the project tasks and press Enter to continue. It will display information about the project.\nNext, it will ask if we want to initialize the project with the above information, enter Y and press Enter.\nThen it will ask us to choose the authentication method we want, we select AWS profile and press Enter. And select the default profile that we configured in the previous step.\nWe will do as shown below.\nThe CLI can determine for itself the appropriate configuration for the project on which Amplify is initialized. In this workshop, CLI knows we are using Vue.js and provides proper configuration with app type, framework, source, distribution, build and start options.\nWhen you initialize a new Amplify project, a few things happen:\nIt will create a new directory named amplify in the root directory of the project and contain the initializations of the backend. It creates a file named aws-exports.js in the src directory of the project. This file contains the necessary configuration information for the services you create with Amplify, this is how Amplify users can get the necessary information of the backend services. Cloud projects created in the AWS Amplify Console can be accessed by running the amplify console command. The console provides a list of backend environments, resources implemented in the Amplify category, the current left page of deployments, and other information. The Amplify initialization process is complete, we will receive the message as shown below.\nNow that our Vue.js app is set up and Amplify is initialized, we\u0026rsquo;re ready to add Amplify hosting in the next steps.\n"
},
{
	"uri": "/5-deploy/5.4-buildapp/",
	"title": "Build the web application",
	"tags": [],
	"description": "",
	"content": "\rEnsure you are in the root of the web project (/webapp):\ncd ~/environment/serverless-tasks-webapp/webapp Install the dependencies:\nnpm install You should see output like this:\nYou can safely ignore any npm install warnings about security vulnerabilities for the purposes of this workshop. In a production environment, you should work to mitigate these.\nBuild the web application:\nnpm run build You should see output like this:\n"
},
{
	"uri": "/4-apigateway/",
	"title": "Configure API authorization: API Gateway",
	"tags": [],
	"description": "",
	"content": "What is Lambda Authorizer? A Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. It is a way to add additional security to your API.\nHow does a Lambda Authorizer work? When a client makes a request to one of your API\u0026rsquo;s methods, API Gateway calls your Lambda authorizer, which takes the caller\u0026rsquo;s identity as input and returns an IAM policy as output.\nAPI Gateway supports open standards for authentication strategies such as OAuth and SAML.\nTypically, an identity provider is used to authenticate the caller. The identity provider is responsible for verifying the caller\u0026rsquo;s identity and returning the caller\u0026rsquo;s identity. The caller\u0026rsquo;s identity is then passed to the Lambda authorizer, which is responsible for controlling access to your API.\nIn this workshop, we won\u0026rsquo;t be implementing the the identity provider needed to vend security tokens, so we will instead be mocking the identity provider. More on this later.\nJSON Web Token (JWT) The caller\u0026rsquo;s identity is represented by a JSON Web Token (JWT). A JWT is a compact, self-contained, signed JSON object. The JWT is typically used to represent an authenticated user\u0026rsquo;s identity.\nExample JWT A JWT token consists of three parts: a header, a payload, and a signature. The header and payload are JSON objects. The signature is a digital signature that is used to verify the header and payload.\nHeader: The type (typ) of token is JWT. The algorithm (alg) is HS256, which is HMAC with SHA-256.\n{\r\u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34;,\r\u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;\r} Payload: The iss attribute stores the identity provider\u0026rsquo;s name. the sub attribute stores the caller\u0026rsquo;s identity. The scope attribute stores the caller\u0026rsquo;s permissions. The jti attribute is a unique identifier for the JWT. The iat attribute stores the time at which the JWT was issued. The exp attribute stores the time at which the JWT expires.\n{\r\u0026#34;iss\u0026#34;: \u0026#34;getting-started-with-serverless\u0026#34;,\r\u0026#34;sub\u0026#34;: \u0026#34;minhnghia\u0026#34;,\r\u0026#34;scope\u0026#34;: \u0026#34;admins\u0026#34;,\r\u0026#34;jti\u0026#34;: \u0026#34;02343566e-8ff3-4a2d-ac18-c4d28ed96881\u0026#34;,\r\u0026#34;iat\u0026#34;: 1633433217,\r\u0026#34;exp\u0026#34;: 4070208800\r} Signature: R2D2RfY_n7OAcuONbpPfxqBY5IppEiGLCCfeQ_wz_2w Các giá trị này đều được mã hóa dạng base64 và nối với form của JWT bằng dấu \u0026ldquo;.\u0026rdquo;, ví dụ như sau:\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJsb2dnZWRJbkFzIjoiYWRtaW4iLCJpYXQiOjE0MjI3Nzk2Mzh9.gzSraSYS8EXBxLN_oWnFSRgCzcmJmMjLiuyu5CSpyHI You will see this token passed by the web application to the API Gateway endpoint using an HTTP Authorization header.\nConfigure a Lambda Authorizer Copy the code below and paste it into the app.js file in the sam/src/auth directory.\nconst jwt = require(\u0026#39;njwt\u0026#39;)\rexports.handler = function (event, context, callback) {\rconsole.info(\u0026#39;received:\u0026#39;, event)\rconst token = event.authorizationToken.split(\u0026#39; \u0026#39;)[1]\rjwt.verify(token, \u0026#39;secretphrase\u0026#39;, (err, verifiedJwt) =\u0026gt; {\rif (err) {\rconsole.log(err.message)\rcallback(\u0026#39;Error: Invalid token\u0026#39;)\r} else {\rconsole.log(`Verified token: ${verifiedJwt}`)\rconst resource = `${event.methodArn.split(\u0026#39;/\u0026#39;, 2).join(\u0026#39;/\u0026#39;)}/*`\rconst policy = generatePolicy(verifiedJwt.body.sub, \u0026#39;Allow\u0026#39;, resource)\rconsole.log(`Generated policy: ${JSON.stringify(policy)}`)\rcallback(null, policy)\r}\r})\r}\rconst generatePolicy = function (principalId, effect, resource) {\rconst authResponse = {}\rauthResponse.principalId = principalId\rif (effect \u0026amp;\u0026amp; resource) {\rconst policyDocument = {}\rpolicyDocument.Version = \u0026#39;2012-10-17\u0026#39;\rpolicyDocument.Statement = []\rconst statementOne = {}\rstatementOne.Action = \u0026#39;execute-api:Invoke\u0026#39;\rstatementOne.Effect = effect\rstatementOne.Resource = resource\rpolicyDocument.Statement[0] = statementOne\rauthResponse.policyDocument = policyDocument\r}\rauthResponse.context = {\ruserId: 1,\rcreatedAt: new Date().toISOString()\r}\rreturn authResponse\r} The code above will verify and authenticate the requests sent to the API using JSON Web Token (JWT) and return an IAM policy for that authenticated person.\nStarting off, we\u0026rsquo;ll import the njwt library, which is used for JWT authentication.\nexports.handler is the entry point of the Lambda function. It will accept an event, a context and a callback. Event contains information about the request sent to the API. Context contains information about the Lambda function. The callback will be called when the Lambda function completes.\nThe JWT token is extracted from the authorizationToken in the request header. There will be a split method used to remove the Bearer prefix and retrieve the actual token.\nThe jwt.verify function is used to verify the token using secretphrase. If the token is invalid, the Lambda function will return an error. If the token is valid, the authenticated JWT object is retrieved and disposed of.\nThe resource variable is set to represent the resources requested from the API request.\nThe generatePolicy function is used to generate an IAM policy. It will take the value of sub (subject) from the body of the JWT, convert effect to Allow and use the resource variable to clarify that policy.\nIn summary, the code above is a simple token-based authentication method, it is used to describe how to use the token to authenticate the request. The token is passed using the header of Authorization. Then we will verify the token using (secretphrase). It\u0026rsquo;s a very common passphrase, applied in web application. We are simply mock identity provider. In practice, we will use a reputable identity provider, such as Amazon Cognito, to verify the user\u0026rsquo;s identity.\nIAM Policy Document This is an example IAM policy document that is returned to the caller. Here, principal minhnghia is granted access to the API Gateway method.\n{\r\u0026#34;principalId\u0026#34;: \u0026#34;minhnghia\u0026#34;,\r\u0026#34;policyDocument\u0026#34;: {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Action\u0026#34;: \u0026#34;execute-api:Invoke\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:execute-api:us-east-1:0123456789012:dn3fidb5ng0/v1/*\u0026#34;\r}\r]\r},\r\u0026#34;context\u0026#34;: {\r\u0026#34;userId\u0026#34;: 1,\r\u0026#34;createdAt\u0026#34;: \u0026#34;2021-10-01T19:53:34.594Z\u0026#34;\r}\r} principalId represents the indentify of the user assigned with the request. In this code, it is assigned the value minhnghia, this IAM policy is assigned to this user with the same identifier as minhnghia.\nobject policyDocument defines the permissions assigned in the policy. Version is the version of the currently used policy language 2012-10-17. Statement is an array of permissions assigned to the user. In this code, it has only one permission, execute-api:Invoke. Effect is either Allow or Deny. Resource is a unique identifier for a particular resource. In this code, it is an identifier for an API Gateway endpoint.\ncontext is an object containing additional information about the user. It can be used to pass additional information about the user to the Lambda function. In this code, it contains two pieces of information, userId and createdAt. This is beneficial in debugging and checking the requests of that user.\n"
},
{
	"uri": "/2-prerequiste/2.4-installamplifycli/",
	"title": "Install Amplify CLI",
	"tags": [],
	"description": "",
	"content": "According to AWS, Amplify is a service designed to help developers build web and mobile applications faster. Amplify provides a set of tools and services for building web and mobile applications. Amplify CLI is part of the Amplify Framework, which provides commands to create and manage AWS resources such as Amazon Cognito, Amazon API Gateway, Amazon Lambda function, Amazon DynamoDB table, and Amazon S3 bucket.\nIn this workshop, we will only configure AWS Amplify Hosting, which provides an easily manageable service for deploying applications global static web application. It will use Amazon S3 and Amazon CloudFornt to store and deliver the static resources of that application.\nInstall the latest version of Amplify CLI by running the following command: npm i -g @aws-amplify/cli Complete the installation of Amplify CLI.\nOnce the download is complete, configure the Amplify CLI: amplify configure We will get a link https://console.aws.amazon.com/ to login to the AWS console.\nAfter logging in, press Enter and select Region that you are doing the workshop, my name is ap-southeast-1. Then press Enter to continue, we we will get a path https://console.aws.amazon.com/iamv2/home#/users/create to create a new IAM user.\nAccess the above link, name amplify-user for that user, select Next.\nBy default, we will assign AdministratorAccess permission to the user configured for Amplify CLI\nIn the Permmissions options section, select Attach policies directly. Find and select AdministratorAccess. Then click Next. On the Review and create page, select Create user.\nNext, select the newly created user amplify-user, select the Security credentials tab, under Access keys, click on Create access key.\nIn step Access key best practices \u0026amp; alternatives, select Command Line Interface (CLI) and tick Confirm then select Next.\nOn the page Set description tag, enter a description for Access key depending on your preference, leave the default and select Create access key. So you have successfully created ** Access key** for user amplify-user, save Access key and Secret access key. Go back to terminal in Cloud9, press Enter, it will ask you enter Access key and Secret access key just created. After entering, press Enter, in Profile name, I will leave default, press Enter, so I have successfully configured Amplify CLI.\nNow we are ready to use Amplify!\n"
},
{
	"uri": "/5-deploy/5.5-amplifyhosting/",
	"title": "Add Amplify hosting",
	"tags": [],
	"description": "",
	"content": "You\u0026rsquo;ve successfully built your first app with Amplify! Now that you\u0026rsquo;ve built something, it\u0026rsquo;s time to deploy it to the web with Amplify Console!\nAll of your static web content including HTML, CSS, JavaScript, images and other files will be managed by AWS Amplify Console. Your end users will then access your site using the public website URL exposed by AWS Amplify Console. You don\u0026rsquo;t need to run any web servers or use other services in order to make your site available.\nFor most real applications you\u0026rsquo;ll want to use a custom domain to host your site. If you\u0026rsquo;re interested in using your own domain, hãy theo dõi cách thiết lập custom domain trên Amplify\nRun the command below to add Amplify Hosting to your web application:\namplify add hosting When it asks you to choose the plugin to execute, select Hosting with Amplify Console. Next, choose the deployment method, you choose Manual deployment. So we have added Amplify Hosting to our web application, please refer to the image below.\nIn the next step, we will publish your app to the web.\n"
},
{
	"uri": "/5-deploy/",
	"title": "Build and deploy a web application: AWS Amplify",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you\u0026rsquo;ll build and the deploy the serverless application stack using SAM. You\u0026rsquo;ll then configure the web application to communicate with an API Gateway endpoint. Then, you\u0026rsquo;ll build the web application, and configure AWS Amplify to host the static resources for your web application.\nNội dung Deploy Serverless Application Configure the API Gateway endpoint Initialize Amplify Build the web application Add Amplify hosting Deploy the application "
},
{
	"uri": "/5-deploy/5.6-deployapp/",
	"title": "Deploy the application",
	"tags": [],
	"description": "",
	"content": "Run the following command to publish your app.\namplify publish And if you get the output like the picture, you have successfully deployed your application to the web.\nAfter publishing, at the bottom of the terminal page will display the URL of our app with the domain amplifyapp.com\nYou can login with any username and password you want because there is no authentication on the backend.\nIf you want to learn how to add authentication to your app, go here\nThe main page of our application will look like this.\nWhenever we want to change something in the application and publish it, just run the amplify publish command again. We won\u0026rsquo;t need to modify the web application, just modify the SAM template and Lambda functions.\nTo manage the application and configure hosting in Amplify Console, run the command amplify console.\n"
},
{
	"uri": "/6-test/",
	"title": "Test the application",
	"tags": [],
	"description": "",
	"content": "\nAfter logging in with any username or password, you can create a new task by entering Title and Body. The Due date part can be left blank.\nClick Create to create a new task, it will be displayed in My Tasks.\nAs you will see in the image below.\nWhen you click the Create task button, things work:\nRequest with POST method is sent to API Gateway endpoint /tasks. The request is authenticated by the Lambda authorizer, using the HTTP header Authentication: Bearer \u0026lt;token\u0026gt;. The body of that request is passed to the Lambda function CreateTaskFunction. The task information entered above will be stored in the DynamoDB table. An HTTP status code 200 response will be returned to the web application. The web application refreshes the list of tasks because the request with the GET method is sent to the API Gateway endpoint /tasks. Again, the request is authenticated by the Lambda authorizer and passed to the Lambda function GetTasksFunction. Lambda function GetTasksFunction retrieves the list of tasks from the DynamoDB table and returns the list of tasks to the authenticated user. Let\u0026rsquo;s create more new tasks and try deleting one to get a better understanding of how requests are processed.\nContent Viewing the data in DynamoDB Logging and Monitoring "
},
{
	"uri": "/7-rekognition/",
	"title": "Configure image metadata extraction: Amazon Rekognition",
	"tags": [],
	"description": "",
	"content": "What is Amazon Rekognition? Amazon Rekognition makes it easy to add image and video analysis to your applications using deep learning technology that requires no machine learning expertise to use. With Amazon Rekognition, you can identify objects, people, text, scenes, and activities in images and videos.\nIn this step, we will add a feature to detect objects in an uploaded image and apply labels to the detected objects.\nConfigure Amazon Rekognition Integration Go to the sam/src/handlers/detectLabels folder and open the app.js file. Copy the code below and paste it into that file:\nconst { DynamoDBClient } = require(\u0026#39;@aws-sdk/client-dynamodb\u0026#39;)\rconst { DynamoDBDocumentClient, UpdateCommand } = require(\u0026#39;@aws-sdk/lib-dynamodb\u0026#39;)\rconst { RekognitionClient, DetectLabelsCommand } = require(\u0026#39;@aws-sdk/client-rekognition\u0026#39;)\rconst ddbClient = new DynamoDBClient()\rconst ddbDocClient = DynamoDBDocumentClient.from(ddbClient)\rconst rekognitionClient = new RekognitionClient()\rconst tableName = process.env.TASKS_TABLE\rexports.handler = async (event) =\u0026gt; {\rconsole.info(JSON.stringify(event, null, 2))\rconst bucket = event.Records[0].s3.bucket.name\rconst key = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \u0026#39; \u0026#39;))\rconst user = key.split(\u0026#39;/\u0026#39;)[0]\rconst taskId = key.split(\u0026#39;/\u0026#39;)[1]\rconst command = new UpdateCommand({\rTableName: tableName,\rKey: { user: `user#${user}`, id: `task#${taskId}` },\rUpdateExpression: \u0026#39;SET upload = :u\u0026#39;,\rExpressionAttributeValues: {\r\u0026#39;:u\u0026#39;: `s3://${bucket}/${key}`\r},\rReturnValues: \u0026#39;UPDATED_NEW\u0026#39;\r})\rconsole.log(`UpdateCommand: ${JSON.stringify(command, null, 2)}`)\rtry {\rconsole.log(`Saving upload for task ${taskId}: s3://${bucket}/${key}`)\rconst data = await ddbDocClient.send(command)\rconsole.log(\u0026#39;UpdateItem succeeded:\u0026#39;, JSON.stringify(data, null, 2))\r} catch (err) {\rconsole.log(\u0026#39;Unable to update item. Error JSON:\u0026#39;, JSON.stringify(err, null, 2))\rthrow err\r}\rconsole.log(`Detecting labels for bucket ${bucket} and key ${key}`)\rconst imageParams = {\rImage: {\rS3Object: {\rBucket: bucket,\rName: key\r}\r}\r}\rconst labelData = await rekognitionClient.send(\rnew DetectLabelsCommand(imageParams)\r)\rconsole.log(\u0026#39;Success, labels detected.\u0026#39;, labelData)\rconst labels = []\rfor (let j = 0; j \u0026lt; labelData.Labels.length; j++) {\rconst name = labelData.Labels[j].Name\rlabels.push(name)\r}\rconsole.log(`Label data: ${JSON.stringify(labels)}`)\rconst updateLabelsCommand = new UpdateCommand({\rTableName: tableName,\rKey: { user: `user#${user}`, id: `task#${taskId}` },\rUpdateExpression: \u0026#39;SET labels = :s\u0026#39;,\rExpressionAttributeValues: {\r\u0026#39;:s\u0026#39;: labels\r},\rReturnValues: \u0026#39;UPDATED_NEW\u0026#39;\r})\rtry {\rconsole.log(`Saving labels for task ${taskId}: ${labels}`)\rconst data = await ddbDocClient.send(updateLabelsCommand)\rconsole.log(\u0026#39;UpdateItem succeeded:\u0026#39;, JSON.stringify(data, null, 2))\r} catch (err) {\rconsole.log(\u0026#39;Unable to update item. Error JSON:\u0026#39;, JSON.stringify(err, null, 2))\rthrow err\r}\rconst response = {\rstatusCode: 200,\rheaders: {\r\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;\r},\rbody: JSON.stringify(labels)\r}\rreturn response\r} The main function of the above code:\nThe code imports the AWS SDK and initializes the clients for DynamoDB and Rekognition. Take the environment variable TASKS_TABLE to create a variable named tableName. The function extracts information related to the event object. It retrieves the buckect and filename of the object uploaded in the S3 bucket. It separates the keys to get the user and task ID associated with the uploaded object. UpdateCommand is created to update items in DynamoDB. Commands are logged in the console for debugging purposes. const command = new UpdateCommand({\rTableName: tableName,\rKey: { user: `user#${user}`, id: `task#${taskId}` },\rUpdateExpression: \u0026#39;SET upload = :u\u0026#39;,\rExpressionAttributeValues: {\r\u0026#39;:u\u0026#39;: `s3://${bucket}/${key}`\r},\rReturnValues: \u0026#39;UPDATED_NEW\u0026#39;\r})\rconsole.log(`UpdateCommand: ${JSON.stringify(command, null, 2)}`) Function runs to UpdateCommand to update the item in DynamoDB. It logs a message when it saves the uploaded object\u0026rsquo;s information and the object\u0026rsquo;s URL. If the update is successful, the response will be logged. Otherwise, an error will be logged and thrown. try {\rconsole.log(`Saving upload for task ${taskId}: s3://${bucket}/${key}`)\rconst data = await ddbDocClient.send(command)\rconsole.log(\u0026#39;UpdateItem succeeded:\u0026#39;, JSON.stringify(data, null, 2))\r} catch (err) {\rconsole.log(\u0026#39;Unable to update item. Error JSON:\u0026#39;, JSON.stringify(err, null, 2))\rthrow err\r} The function is executed to detect the labels of images uploaded using Amazon Rekognition. It logs the message of the detec label action in the bucket. Object imageParams is created to identify the image (S3 object) detected label. DetectLabelsCommand is created to call the Rekognition API to detect the label of the image. The detected labels are stored in the array of labels. console.log(`Detecting labels for bucket ${bucket} and key ${key}`)\rconst imageParams = {\rImage: {\rS3Object: {\rBucket: bucket,\rName: key\r}\r}\r}\rconst labelData = await rekognitionClient.send(\rnew DetectLabelsCommand(imageParams)\r)\rconsole.log(\u0026#39;Success, labels detected.\u0026#39;, labelData)\rconst labels = []\rfor (let j = 0; j \u0026lt; labelData.Labels.length; j++) {\rconst name = labelData.Labels[j].Name\rlabels.push(name)\r} In short, this Lambda function handles events when an object is uploaded in the S3 bucket. It updates the item in DynamoDB with the uploaded information as the S3 URL. It then uses Amazon Rekognition to detect labels in the image and updates the item in DynamoDB with the detected labels. The function responds with a JSON object that includes detected labels.\nNext we need to add a new resource AWS::Serverless::Function to the template.yaml file in the sam folder, in the Resources, section under the **UploadsBucket function. ** Copy and paste the code below into the template.yaml file in the sam. folder\n# DetectLables Lambda Function\rDetectLabelsFunction:\rType: AWS::Serverless::Function\rProperties:\rCodeUri: src/handlers/detectLabels\rEnvironment:\rVariables:\rTASKS_TABLE: !Ref TasksTable\rEvents:\rObjectCreatedEvent:\rType: S3\rProperties:\rBucket: !Ref UploadsBucket\rEvents: s3:ObjectCreated:*\rHandler: app.handler\rPolicies:\r- DynamoDBCrudPolicy:\rTableName: !Ref TasksTable\r- RekognitionDetectOnlyPolicy: {}\r- Version: 2012-10-17\rStatement:\r- Effect: Allow\rAction: s3:GetObject*\rResource: !Sub \u0026#34;arn:aws:s3:::uploads-${AWS::StackName}-${AWS::Region}-${AWS::AccountId}*\u0026#34;\rRuntime: nodejs14.x They will do as shown below:\nNotice in the ObjectCreatedEvent. property This event will trigger the Lambda function when an object is uploaded to the bucket.\nDeploy the changes With our code change editing the template.yml file and adding the detectLabels.js, file we need to build and redeploy the application to implement the changes. We will deploy running the following command:\nMake sure that in terminal you are in sam directory, run this command cd ~/environment/serverless-tasks-webapp/sam\rFirst we will build the application:\nsam build When the build is successful, we will see something like this:\nNext, we will deploy the application:\nsam deploy After successfully deploying, we will see as shown\nNow, we can experience Amazon Rekognition in our application.\nAmazon Rekognition will analyze images stored in the Amazon S3 bucket. Our web application allows you to upload images and assign them to tasks. We do the following:\nAfter you create a new task, in the My Tasks section, you will see it and the Choose file. button Click on that button to select an image from your computer with the PNG extension. , JPEG, etc. Click the Upload button to upload the image. After doing the above steps, you will see the line showing the Detecting Labels. process\nAfter the detection process is complete, you will see the detected labels displayed.\nWhat just happened? Images uploaded to S3 bucket The S3 bucket is configured to enable the Lambda function DetectLabels trigger when an object is created in the bucket. Lambda function DetectLabels is called and it uses Amazon Rekognition to detect the label of the image. Detected lables are saved to DynamoDB. The application refreshes the details of the task. Notice how we add a new feature to our application without changing too much. This demonstrates the flexibility of serverless architectures - its scalability and modularity.\n"
},
{
	"uri": "/8-terminate/",
	"title": "Terminate Resources",
	"tags": [],
	"description": "",
	"content": "SAM-created Resources The simplest way to delete resources created by SAM is to use the sam delete command. However, before running that command, you need to make sure that the S3 bucket containing the uploaded images has been cleaned up.\nUse the command aws s3 ls to see the list of files in the bucket. Please do as shown.\nYou should see a bucket named uploads-tasks-app-ap-southeast-1-127779471063 where ap-southeast-1 is your region and 127779471063 is the ID of your AWS account.\nGo to the /sam directory with the command cd ~/environment/serverless-tasks-webapp/sam. The bucket for this workshop is versioning enabled so you have to clean it up using the AWS SDK. Find the emtpy_versioned_bucket.py file in the /sam directory and modify it by editing the bucket name above after running the command aws s3 ls. Save the file and run the following command to clean up the bucket:\npython empty_versioned_bucket.py We will have a result like this:\nNow we can run the command:\nsam delete So we have successfully deleted the resources generated by SAM.\nAmplify Hosting Di chuyển vào thư mục /webapp, chúng ta có thể xóa Amplify project bằng câu lệnh:\ncd ~/environment/serverless-tasks-webapp/webapp\ramplify delete After successful deletion, we will see as shown:\nRemember to go to the IAM console, select the user amplify-user created in step 2.4 - Installing Amplify CLI and delete it with the generated Access Key output because it is not automatically deleted when amplify delete is run.\nCloudWatch Logs You must delete the log groups in CloudWatch Logs and streams generated while using the application. Go to CloudWatch Logs console, then go to Log groups in the left navigation bar, select this workshop\u0026rsquo;s Log groups and delete them.\nSo we\u0026rsquo;ve cleaned up all the resources in this workshop.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]